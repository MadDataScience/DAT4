{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Science\n",
      "====\n",
      "### The World is Data\n",
      "\n",
      "Alessandro Gagliardi  \n",
      "Sr. Data Scientist, [Glassdoor.com](Glassdoor.com)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Last Time:\n",
      "\n",
      "- #### What is Data Science?\n",
      "- #### Goals of the Course   \n",
      "- #### Python Intro\n",
      "\n",
      "#### Questions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "- Slides, etc.\n",
      "- Projects\n",
      "- Schoology & Discussions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Agenda\n",
      "\n",
      "1. Lab I\n",
      "    1. Amazon Web Services\n",
      "2. Lecture  \n",
      "    1. Big Data  \n",
      "    2. MapReduce Programming Model  \n",
      "    3. Implementation Details  \n",
      "    4. Word Count Example  \n",
      "3. Lab II\n",
      "    2. Hadoop Streaming  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Lab I: Amazon Web Services (AWS)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "A. Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As you have probably heard, **big data** is a hot topic these days."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Q:  What does \u201cbig data\u201d actually refer to?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A: Three \u2018V\u2019s:  \n",
      "\n",
      "-\tVolume\n",
      "-\tVelocity\n",
      "-\tVariety"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "- Volume (i.e. more than can fit in memory on your laptop)  \n",
      "    - e.g. Amazon\u2019s user behavior data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Velocity (i.e. faster than standard machines can process)\n",
      "    - e.g. Twitter\u2019s \u201cFirehose\u201d of tweets\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- Variety (i.e. does not conform to a single structure)\n",
      "\t- e.g. Google\u2019s cache of web pages\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "- Volume (i.e. more than can fit in memory on your laptop)  \n",
      "    - e.g. Amazon\u2019s user behavior data  \n",
      "    \n",
      "    <br />\n",
      "    \n",
      "- ~~Velocity (i.e. faster than standard machines can process)~~\n",
      "    - ~~e.g. Twitter\u2019s \u201cFirehose\u201d of tweets~~  \n",
      "    \n",
      "    <br />    \n",
      "    \n",
      "- Variety (i.e. does not conform to a single structure)\n",
      "\t- e.g. Google\u2019s cache of web pages\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "In this course, we will not deal with velocity. That is really more of an engineering problem than a data science problem. In fact, all three of these are engineering problems, but only the first and the third are really germane to Data Science. That said, as a Data Scientist, it is probably only a matter of time before someone asks you to create a real-time dashboard. Such a thing, while cool, is often very difficult to create and has questionable utility for anyone besides an operations engineer or a day trader, and they already have real-time dashboards."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "How would you approach dealing with this kind of data?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "One approach would be to get a huge supercomputer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "But this has some obvious drawbacks:\n",
      "\n",
      "- expensive\n",
      "- difficult to maintain\n",
      "- scalability is bounded"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Instead of one huge machine, what if we got a bunch of regular (*commodity*) machines?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This has obvious benefits!\n",
      "\n",
      "- cheaper\n",
      "- easier to maintain\n",
      "- scalability is unbounded (just add more nodes to the *cluster*)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We can visualize this *horizontal* cluster architecture as a single client-multiple server relationship:  \n",
      "\n",
      "<img src=http://star.mit.edu/cluster/docs/latest/_images/scoverview.png />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "There are two ways to process data in a distributed architecture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "1. move data to code (& processing power)\n",
      "    - SETI\n",
      "2. move code to data\n",
      "    - map-reduce -> less overhead (network traffic, disk I/O)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\u201cComputing nodes are the same as storage nodes.\u201d"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Divide and conquer is a fundamental algorithmic technique for solving a given task, whose steps include:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "1. split task into subtasks\n",
      "2. solve these subtasks independently\n",
      "3. recombine the subtask results into a final result"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "One famous example of divide and conquer is *merge sort*.\n",
      "<CENTER><IMG SRC=\"http://upload.wikimedia.org/wikipedia/commons/e/e6/Merge_sort_algorithm_diagram.svg\" /></CENTER>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The defining characteristic of a problem that is suitable for the divide and conquer approach is that it can be broken down into independent subtasks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Tasks that can be parallelized in this way include:\n",
      "\n",
      "- count, sum, average\n",
      "- grep, sort, inverted index\n",
      "- graph traversals, some ML algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Let\u2019s do a toy example with average."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "B. Programming Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As we\u2019ve discussed, the map-reduce approach involves splitting a problem into subtasks and processing these subtasks in parallel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This takes place in two phases:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "1. the **mapper** phase\n",
      "2. the **reducer** phase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "As we\u2019ve discussed, the map-reduce approach involves splitting a problem into subtasks and processing these subtasks in parallel.\n",
      "\n",
      "This takes place in (*approximately*) two phases:\n",
      "\n",
      "1.  the **mapper** phase  \n",
      "2.  *shuffle/sort*    <-    the secret sauce  \n",
      "3.  the **reducer** phase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<CENTER><img src=http://mikecvet.files.wordpress.com/2010/07/mrfigure.png /></CENTER>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Map-reduce uses a *functional programming* paradigm. The data processing *primitives* are mappers and reducers, as we\u2019ve seen."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**mappers** \u2013 filter & transform data  \n",
      "**reducers** \u2013 aggregate results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = [1, 2, 3]\n",
      "b = [4, 5, 6, 7]\n",
      "c = [8, 9, 1, 2, 3]\n",
      "L = map(lambda x:len(x), [a, b, c])\n",
      "L"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "[3, 4, 5]"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = reduce(lambda x, y: x+y, L)\n",
      "N"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "12"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Or, if we want to be fancy and do it in one line\n",
      "reduce(lambda x, y: x+y, map(lambda x:len(x), [a, b, c]))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "12"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Thanks to [Michael Cvet](http://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/) for this example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Map-reduce uses a functional programming paradigm. The data processing primitives are mappers and reducers, as we\u2019ve seen.  \n",
      "\n",
      "**mappers** \u2013 filter & transform data  \n",
      "**reducers** \u2013 aggregate results  \n",
      "\n",
      "The functional paradigm is good at describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "It\u2019s possible to overlay the map-reduce framework with an additional *declarative syntax*.\n",
      "\n",
      "This makes operations like select & join easier to implement and less error prone.\n",
      "\n",
      "Popular examples for Hadoop include Pig and Hive."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Why Pig?\n",
      "Because I bet you can read the following script."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "An example pig script:\n",
      "\n",
      "```pig\n",
      "users = load 'users.csv' as (username: chararray, age: int);\n",
      "users_1825 = filter users by age >= 18 and age <= 25;\n",
      "\n",
      "pages = load 'pages.csv' as (username: chararray, url: chararray);\n",
      "\n",
      "joined = join users_1825 by username, pages by username;\n",
      "grouped = group joined by url;\n",
      "summed = foreach grouped generate group as url, COUNT(joined) AS views;\n",
      "top_5 = limit sorted 5;\n",
      "\n",
      "store top_5 into 'top_5_sites.csv';\n",
      "```\n",
      "\n",
      "Now, just for fun... the same calculation in vanilla Hadoop MapReduce."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/pig_02.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "C. Implementation Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce is\u2026"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A. A programming model for processing big data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "B. A data processing system created and used by Google"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "C. A feature of Hadoop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "D. All of the above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The implementation of MapReduce became popular when Google released a white paper in 2004 explaining how they did it. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This architecture was then copied. The two most popular versions are:\n",
      "\n",
      "- Apache\u2019s Hadoop\n",
      "- Disco (bet you haven\u2019t heard of this one)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Disco more friendly but not as well supported. Hadoop has a huge community and so, while more difficult to use, is the standard."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "A map-reduce framework handles a lot of messy details for you:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- parallelization & distribution (eg, input splitting)\n",
      "- partitioning (shuffle/sort/redirect)\n",
      "- fault-tolerance (fact: tasks/nodes will fail!)\n",
      "- I/O scheduling\n",
      "- status and monitoring"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This (along with the functional semantics) allows you to focus on solving the problem instead of accounting & housekeeping details."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**Hadoop** is a popular open-source Java-based implementation of the map-reduce framework (including file storage for input/output)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You can download Hadoop and configure a set of machines to operate as a map-reduce cluster, or you can run it as a *service* via Amazon\u2019s Elastic Map-Reduce."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Hadoop is written in Java, but the *Hadoop Streaming* utility allows client code to be supplied as executables (eg, written in any language)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data is replicated in the (distributed) file system across several nodes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This permits locality optimization (and fault tolerance) by allowing the mapper tasks to run on the same nodes where the data resides."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "So we move code to data (instead of data to code), thus avoiding a lot of network traffic and disk I/O."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The *Google File System* (GFS) was developed alongside map-reduce to serve as the native file system for this type of processing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The Hadoop platform is bundled with an open-source implementation of this file system called *HDFS*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If you use Amazon EMR, you can use their file system (Amazon S3) as well. \n",
      " *(What's wrong with this?)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Hadoop claims to be \"for the cloud\" but it's not. \n",
      "\n",
      "The point of the cloud is that it's ephemeral. The point of Hadoop is that it's not.\n",
      "\n",
      "If your data is in the cloud, better go with the SETI model (i.e. move data to code).\n",
      "\n",
      "Enter: IPython.parallel"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "D. Word Count Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce processes data in terms of *key-value pairs*:\n",
      "\n",
      "    input           <k1, v1>\n",
      "\n",
      "    mapper          <k1, v1> -> <k2, v2>\n",
      "\n",
      "    (partitioner)   <k2, v2> -> <k2, [all k2 values]>\n",
      "\n",
      "    reducer         <k2, [all k2 values]> -> <k3, v3>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using the following input, we can implement the \u201cHello World\u201d of map-reduce: a *word count*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = {1: 'where',\n",
      "     2: 'where in',\n",
      "     3: 'where in the',\n",
      "     4: 'where in the world',\n",
      "     5: 'where in the world is',\n",
      "     6: 'where in the world is carmen',\n",
      "     7: 'where in the world is carmen sandiego'}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "First, as for comparison, this is one way you could do it on a single machine:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "wordcount = defaultdict(int)\n",
      "\n",
      "for _, s in i:\n",
      "    for w in s.split():\n",
      "        wordcount[w] += 1\n",
      "\n",
      "pd.Series(wordcount)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "carmen      2\n",
        "in          6\n",
        "is          3\n",
        "sandiego    1\n",
        "the         5\n",
        "where       7\n",
        "world       4\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Do employ parallel processing, we need to invoke IPython.parallel and scatter our data to our respective workers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client\n",
      "c = Client()\n",
      "dview = c.direct_view()\n",
      "dview.scatter('i', i)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "<AsyncResult: scatter>"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "N.B. In the real world, we wouldn't scatter the data like this. Rather, we would scatter pointers to the data and let each worker retrieve the data from their source repsectively."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The first processing primitive is the mapper, which filters & transforms the input data, and *yields* transformed key-value pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "from itertools import chain"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapper(k1, v1):\n",
      "    '''k1: line number\n",
      "       v1: line contents (i.e. space-delimited string)'''\n",
      "    words = v1.split()  # split string into words\n",
      "    for word in words:\n",
      "        yield (word, 1)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_results = list(chain.from_iterable(map(mapper, i)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "`%%px` means 'parallel execute'. This is being run on all nodes simultaneously."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The mapper emits key-value pairs for each word encountered in the input data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain\n",
      "map_results = list(chain.from_iterable(map(mapper, i.keys(), i.values())))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[('where', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('carmen', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('carmen', 1),\n",
        " ('sandiego', 1)]"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t :2\n",
      "map_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\u001b[0;31mOut[0:4]: \u001b[0m[('where', 1), ('where', 1), ('in', 1)]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\u001b[0;31mOut[1:5]: \u001b[0m\n",
        "[('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1)]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The partitioner is internal to the map-reduce framework, so we don\u2019t have to write this ourselves. It shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a *single* reducer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "If we were using a map-reduce framework like Hadoop or Disco, the partitioner would be internal. But since we are building this from scratch, we will create our own partitioner using a `defaultdict`. The partitioner shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a *single* reducer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "partition_results = defaultdict(list)\n",
      "\n",
      "#map_results = dview.gather('map_results').get()\n",
      "\n",
      "for k2, v2 in map_results:\n",
      "    partition_results[k2].append(v2)\n",
      "\n",
      "#dview.scatter('partitioner', partitioner.items())\n",
      "\n",
      "partition_results = dict(partition_results)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "partition_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "{'carmen': [1, 1],\n",
        " 'in': [1, 1, 1, 1, 1, 1],\n",
        " 'is': [1, 1, 1],\n",
        " 'sandiego': [1],\n",
        " 'the': [1, 1, 1, 1, 1],\n",
        " 'where': [1, 1, 1, 1, 1, 1, 1],\n",
        " 'world': [1, 1, 1, 1]}"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Finally, the reducer receives all values for a given key and aggregates (in this case, sums) the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "def reducer(k2v2):\n",
      "    k2, v2 = k2v2\n",
      "    return k2, sum(v2)\n",
      "\n",
      "reduce_results = map(reducer, partitioner)\n",
      "\n",
      "reduce_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\u001b[0;31mOut[0:59]: \u001b[0m[('sandiego', 1), ('carmen', 2), ('is', 3)]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\u001b[0;31mOut[1:58]: \u001b[0m[('in', 6), ('world', 4)]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "\u001b[0;31mOut[2:55]: \u001b[0m[('the', 5), ('where', 7)]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducer(k2, k2_vals):\n",
      "    '''k2: word\n",
      "       k2_vals: word counts'''\n",
      "    yield k2, sum(k2_vals)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Reducer output is aggregated & sorted by key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results = dict(chain.from_iterable(map(reducer, partition_results.keys(), partition_results.values())))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "{'carmen': 2,\n",
        " 'in': 6,\n",
        " 'is': 3,\n",
        " 'sandiego': 1,\n",
        " 'the': 5,\n",
        " 'where': 7,\n",
        " 'world': 4}"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.Series(dict(dview.gather('reduce_results').get()))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "carmen      2\n",
        "in          6\n",
        "is          3\n",
        "sandiego    1\n",
        "the         5\n",
        "where       7\n",
        "world       4\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Lab II: Python & Hadoop Streaming"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Discussion"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}