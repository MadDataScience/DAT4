{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Science\n",
      "====\n",
      "### AWS, StarCluster, IPython.parallel, Pandas\n",
      "\n",
      "Alessandro Gagliardi  \n",
      "Sr. Data Scientist, [Glassdoor.com](Glassdoor.com)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Last Time:\n",
      "\n",
      "- #### Big Data\n",
      "- #### MapReduce Programming Model   \n",
      "- #### Implementation Details\n",
      "- #### Word Count Example\n",
      "\n",
      "#### Questions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Agenda:\n",
      "\n",
      "1. Readiness Assement\n",
      "2. Amazon Web Services\n",
      "3. StarCluster\n",
      "4. IPython.parallel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Readiness Assement\n",
      "\n",
      "Two Parts:\n",
      "\n",
      "1. Individual\n",
      "2. Team"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "    AA | CC | EE\n",
      "    BA | DC | FE\n",
      "    BB | DD | FF"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "#### Teams:\n",
      "\n",
      "<TABLE>\n",
      "    <TR><TD bgcolor='red'>A</TD><TD bgcolor='red'>A</TD><TD></TD>\n",
      "        <TD bgcolor='yellow'>C</TD><TD bgcolor='yellow'>C</TD><TD></TD>\n",
      "        <TD bgcolor='blue'>E</TD><TD bgcolor='blue'>E</TD></TR>\n",
      "    <TR><TD bgcolor='purple'>B</TD><TD bgcolor='red'>A</TD><TD></TD>\n",
      "        <TD bgcolor='green'>D</TD><TD bgcolor='yellow'>C</TD><TD></TD>\n",
      "        <TD bgcolor='orange'>F</TD><TD bgcolor='blue'>E</TD></TR>\n",
      "    <TR><TD bgcolor='purple'>B</TD><TD bgcolor='purple'>B</TD><TD></TD>\n",
      "        <TD bgcolor='green'>D</TD><TD bgcolor='green'>D</TD><TD></TD>\n",
      "        <TD bgcolor='orange'>F</TD><TD bgcolor='orange'>F</TD></TR>\n",
      "    <TR></TR>\n",
      "</TABLE>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "## Readiness Assement\n",
      "\n",
      "1. T/F:  \n",
      "    Part of the innovation of MapReduce is that instead of moving code to data, you move data to code.  \n",
      "\n",
      "2. Which of the following can be used as HDFS:\n",
      "    1. Elastic Beanstalk\n",
      "    2. IAM\n",
      "    3. Redshift\n",
      "    4. S3\n",
      "    5. None of the Above  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "3. Write a mapper and reducer (in Python or pseudocode) that takes a list of page visits and yields a list of URLs and the number of unique visitors to those URLs given input like the following:\n",
      "\n",
      "        timestamp       url                     user\n",
      "        201301010000    example.com/page01.html userA\n",
      "        201301010100    example.com/page02.html userA\n",
      "        201302010330    example.com/page01.html userB\n",
      "        201303010400    example.com/page03.html userC\n",
      "        201303010401    example.com/page02.html userA\n",
      "\n",
      "        def mapper(k1, v1):\n",
      "            '''\n",
      "            k1: timestamp\n",
      "            v1: url user (separated by a space)\n",
      "            '''\n",
      "            ...\n",
      "            yield (k2, v2)\n",
      "        \n",
      "        def reducer(k2, k2_vals):\n",
      "            ...\n",
      "            yield (url, number_of_unique_users)\n",
      "\n",
      "The output should look something like:\n",
      "        \n",
      "        example.com/page01.html    2\n",
      "        example.com/page02.html    1\n",
      "        example.com/page03.html    1\n",
      "\n",
      "Hint: `set()` and `len()`, while not necessary, may be helpful."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=assets/AWS.png />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Amazon Web Services\n",
      "\n",
      "- IAM: Identity and Access Management\n",
      "- S3: Simple Storage Service\n",
      "- EC2: Elastic Cloud Compute\n",
      "- EMR: Elastic MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Regions:\n",
      "\n",
      "- us-east-1 (North Virginia)\n",
      "- us-west-1 (Northern California)\n",
      "- us-west-2 (Oregon)\n",
      "- also regions in Asia, Europe, and South America\n",
      "\n",
      "### Availability Zones (AZ):\n",
      "\n",
      "* Each region has several availability zones."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "AZs should rarely matter, but important to know what it is (and not confuse AZs with regions). AZ will rarely matter. Region often will."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Types of authentication:\n",
      "\n",
      "<small>(Warning: the word *key* is used a lot to mean different things.)</small>\n",
      "\n",
      "1. Key & Secret\n",
      "    - Key = 20 character long token (all upper-case)\n",
      "    - Secret = 40 character long string\n",
      "    - Used for all AWS\n",
      "    - Created and managed in IAM\n",
      "    - Not region specific\n",
      "2. Keypair\n",
      "    - RSA encrypted\n",
      "    - Used specifically for SSH into EC2 (including EMR)\n",
      "    - Region specific\n",
      "    - Created and managed in EC2\n",
      "3. Other forms of authentication\n",
      "    - Alternative to Key & Secret\n",
      "    - Managed in IAM\n",
      "    - Useful for granting others partial access to your AWS resources  \n",
      "        (i.e. giving a friend write access to a specific S3 bucket)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### S3 (Simple Storage Service)\n",
      "\n",
      "- Sort of a file system\n",
      "    - Buckets ~ Folders\n",
      "    - Keys ~ Files (Note: a different meaning for the word *key* in this context)\n",
      "    - Subdirectories aren't really folders, but rather just prefixes to the key name\n",
      "- Region Specific\n",
      "    - but it usually doesn't matter (except for latency)\n",
      "- Relatively Cheap\n",
      "- Complex Permission Structure (about as complex as a real file system)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### EC2: Elastic Cloud Compute: Instance Types\n",
      "\n",
      "- Optimization\n",
      "    - Compute Optimized\n",
      "    - Memory Optimized\n",
      "    - Storage Optimized\n",
      "    - General Purpose\n",
      "- Pricing Examples:\n",
      "    - t1.micro\n",
      "        - 1 CPU\n",
      "        - 615 MB RAM\n",
      "        - \\$0.02 per Hour\n",
      "    - i2.8xlarge\n",
      "        - 32 CPUs\n",
      "        - 244 GB RAM\n",
      "        - \\$6.82 per Hour\n",
      "    - m1.large \n",
      "        - 2 CPUs\n",
      "        - 7.5 GB RAM\n",
      "        - \\$0.24 per Hour\n",
      "- Not all instance types available in all regions (e.g. High Storage Instances not available in us-west-1)\n",
      "- Prices may vary (e.g. instances in us-west-1 are often more expensive)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### EC2: Elastic Cloud Compute: Amazon Machine Images (AMIs)\n",
      "\n",
      "* A machine image is a copyable snapshot of an instance's contents and configuration\n",
      "* If you've ever run a virtual machine (e.g. VirtualBox, VMware, Parallels), you may be familiar\n",
      "* EC2 instances start as AMIs that are then *instantiated*\n",
      "* StarCluster uses AMIs extensively to preconfigure EC2 instances to create a cluster\n",
      "* A well-configured AMI can save a lot of time\n",
      "* AMIs are region specific. Copies to other regions receive new ID"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "A well-configured AMI can save a lot of time because you don't have to install software on every instance after it's booted up (the software is effectively pre-installed).\n",
      "They can be copied, but only by the creator and it will get a new ID when that happens."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### EMR: Elastic MapReduce\n",
      "\n",
      "#### Hadoop in the Cloud\n",
      "\n",
      "* Uses EC2 instances\n",
      "* Web interface for provisioning cluster\n",
      "* Installs Hadoop, etc. automatically\n",
      "* Installs Hive, Pig, HBase automatically if desired\n",
      "* Provides a web interface to running Hadoop Streaming jobs. \n",
      "* Charges a small premium on top of EC2 prices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "One thing EMR does that StarCluster doesn't is it automatically configures Hadoop to be able to read your S3 buckets as if they were part of HDFS. This has to be done manually in StarCluster (or any other software that runs Hadoop on EC2 (e.g. Whirr, etc.))\n",
      "\n",
      "We probably won't use EMR anymore in this course. StarCluster works just as well, has more options, and is cheaper. That said, EMR is probably the way most people use Hadoop these days (or at least the way most people use Hadoop on AWS) which is why we covered it last week."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Real Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<a href=http://aws.amazon.com/datasets/Encyclopedic/2506>http://aws.amazon.com/datasets/Encyclopedic/2506</a><br />\n",
      "<image src=assets/wikipedia.png /><br />\n",
      "<H1>500 GB!!!</H1>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# StarCluster  \n",
      "\n",
      "<img src=http://star.mit.edu/cluster/docs/latest/_images/scoverview.png />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "StarCluster comes out of the STAR program at MIT. STAR stands for \"Software Tools for Academics and Researchers\". It is used to quickly provision a cluster of EC2 instances. Like EMR, it automatically configures them to be used as a cluster (rather than as independent machines) with one controller and many workers. Unlike EMR, it does not have a GUI. Also, Hadoop is just one of many plugins for StarCluster. The most important plugin, however, is IPython Cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<a href=https://docs.google.com/spreadsheet/ccc?key=0Asc8NbYa4sgCdGxQVnpQamtkUC1zYXRWcXF1R1lFcmc&usp=sharing>http://tinyurl.com/EC2calc</a><br />\n",
      "<image src=assets/ec2calc.png /><br />\n",
      "<P>For $13.12/hour we can get 8 m2.4xlarge machines with 68.4 GiB Memory a piece.</P>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "File: .starcluster/config"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[cluster mediumcluster]\n",
      "# Declares that this cluster uses smallcluster as defaults\n",
      "EXTENDS=smallcluster\n",
      "# This section is the same as smallcluster except for the following settings:\n",
      "NODE_INSTANCE_TYPE = m2.4xlarge\n",
      "CLUSTER_SIZE=8"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "$ starcluster start -c mediumcluster wikipedia"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "$ starcluster put wikipedia --user sgeadmin ~/Downloads/credentials.csv /home/sgeadmin/"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=assets/ipcluster.png />"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Parse Wikipedia Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Next Time:\n",
      "\n",
      "1. APIs & JSON\n",
      "2. NoSQL Databases\n",
      "\n",
      "### Homework:\n",
      "\n",
      "1. Create a [Twitter]( Account (if you don't already have one)\n",
      "2. Install [MongoDB](http://www.mongodb.org/downloads)\n",
      "3. Install Python Packages (i.e. conda install ... or pip install ...)\n",
      "    - [PyMongo](https://pypi.python.org/pypi/pymongo)\n",
      "    - [Twitter](https://pypi.python.org/pypi/twitter)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}